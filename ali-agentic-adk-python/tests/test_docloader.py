from __future__ import annotations

import io
import os
import json
from pathlib import Path

import pytest
from PyPDF2 import PdfWriter

from ali_agentic_adk_python.core import (
    Document,
    MarkdownDocLoader,
    PDFDocLoader,
    RecursiveCharacterTextSplitter,
    TextDocLoader,
)


# ============================================================================
# TextDocLoader åŸºç¡€æµ‹è¯•
# ============================================================================

def test_text_doc_loader_reads_file(tmp_path):
    path = tmp_path / "sample.txt"
    path.write_text("Hello ADK!", encoding="utf-8")

    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()

    assert len(documents) == 1
    doc = documents[0]
    assert doc.page_content == "Hello ADK!"
    assert doc.metadata["source"] == str(path)


def test_text_doc_loader_raises_when_path_missing():
    loader = TextDocLoader()
    with pytest.raises(ValueError):
        loader.load()


def test_text_doc_loader_fetches_from_metadata_stream():
    loader = TextDocLoader()
    stream = io.StringIO("From stream")

    documents = loader.fetch_content({"stream": stream, "metadata": {"category": "demo"}})

    assert documents[0].page_content == "From stream"
    assert documents[0].metadata["source"] == "stream"
    assert documents[0].metadata["category"] == "demo"


def test_text_doc_loader_fetches_from_metadata_path(tmp_path):
    path = tmp_path / "note.txt"
    path.write_text("metadata path", encoding="utf-8")

    loader = TextDocLoader()
    documents = loader.load_with_meta({"file_path": str(path)})

    assert documents[0].page_content == "metadata path"
    assert documents[0].metadata["source"] == str(path)


def test_text_doc_loader_load_and_split_preserves_metadata(tmp_path):
    path = tmp_path / "long.txt"
    content = "alpha beta gamma delta epsilon zeta eta theta"
    path.write_text(content, encoding="utf-8")

    loader = TextDocLoader(str(path))
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=15, max_chunk_overlap=3)

    chunks = loader.load_and_split(splitter)

    assert len(chunks) > 1
    assert {chunk.metadata["source"] for chunk in chunks} == {str(path)}
    combined = "".join(chunk.page_content for chunk in chunks).replace(" ", "")
    assert combined == content.replace(" ", "")


# ============================================================================
# TextDocLoader ç¼–ç æµ‹è¯•
# ============================================================================

def test_text_doc_loader_reads_utf8_with_bom(tmp_path):
    """æµ‹è¯•è¯»å–å¸¦BOMçš„UTF-8æ–‡ä»¶"""
    path = tmp_path / "utf8_bom.txt"
    content = "UTF-8 with BOM: ä½ å¥½ä¸–ç•Œ"
    path.write_bytes(b'\xef\xbb\xbf' + content.encode('utf-8'))
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert len(documents) == 1
    assert "ä½ å¥½ä¸–ç•Œ" in documents[0].page_content


def test_text_doc_loader_reads_chinese_characters(tmp_path):
    """æµ‹è¯•è¯»å–ä¸­æ–‡å­—ç¬¦"""
    path = tmp_path / "chinese.txt"
    content = "è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡æµ‹è¯•æ–‡ä»¶ï¼ŒåŒ…å«ä¸­æ–‡å­—ç¬¦ã€‚"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert documents[0].page_content == content
    assert "ä¸­æ–‡" in documents[0].page_content


def test_text_doc_loader_reads_emoji(tmp_path):
    """æµ‹è¯•è¯»å–emojiè¡¨æƒ…"""
    path = tmp_path / "emoji.txt"
    content = "Hello ğŸ‘‹ World ğŸŒ ADK ğŸš€"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert documents[0].page_content == content


def test_text_doc_loader_reads_mixed_languages(tmp_path):
    """æµ‹è¯•è¯»å–æ··åˆè¯­è¨€æ–‡æœ¬"""
    path = tmp_path / "mixed.txt"
    content = "English è‹±æ–‡ æ—¥æœ¬èª í•œêµ­ì–´ EspaÃ±ol FranÃ§ais"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert documents[0].page_content == content


# ============================================================================
# TextDocLoader ç©ºå†…å®¹å’Œç‰¹æ®Šå†…å®¹æµ‹è¯•
# ============================================================================

def test_text_doc_loader_reads_empty_file(tmp_path):
    """æµ‹è¯•è¯»å–ç©ºæ–‡ä»¶"""
    path = tmp_path / "empty.txt"
    path.write_text("", encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert len(documents) == 1
    assert documents[0].page_content == ""


def test_text_doc_loader_reads_whitespace_only(tmp_path):
    """æµ‹è¯•åªåŒ…å«ç©ºç™½å­—ç¬¦çš„æ–‡ä»¶"""
    path = tmp_path / "whitespace.txt"
    path.write_text("   \n\n   \t\t   \n", encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert len(documents) == 1
    assert documents[0].page_content.strip() == ""


def test_text_doc_loader_reads_single_line(tmp_path):
    """æµ‹è¯•å•è¡Œæ–‡æœ¬"""
    path = tmp_path / "single_line.txt"
    path.write_text("Single line without newline", encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert documents[0].page_content == "Single line without newline"


def test_text_doc_loader_reads_multiline(tmp_path):
    """æµ‹è¯•å¤šè¡Œæ–‡æœ¬"""
    path = tmp_path / "multiline.txt"
    content = "Line 1\nLine 2\nLine 3\nLine 4\nLine 5"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert documents[0].page_content == content
    assert documents[0].page_content.count("\n") == 4


def test_text_doc_loader_reads_long_lines(tmp_path):
    """æµ‹è¯•è¶…é•¿è¡Œ"""
    path = tmp_path / "long_line.txt"
    content = "x" * 10000
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert len(documents[0].page_content) == 10000


def test_text_doc_loader_reads_special_characters(tmp_path):
    """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦"""
    path = tmp_path / "special.txt"
    content = "Special chars: !@#$%^&*()_+-=[]{}|;:',.<>?/~`"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert documents[0].page_content == content


def test_text_doc_loader_reads_json_content(tmp_path):
    """æµ‹è¯•JSONæ ¼å¼å†…å®¹"""
    path = tmp_path / "data.json"
    data = {"name": "test", "value": 123, "nested": {"key": "value"}}
    path.write_text(json.dumps(data, indent=2), encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    loaded_data = json.loads(documents[0].page_content)
    assert loaded_data == data


# ============================================================================
# TextDocLoader è·¯å¾„å’Œæ–‡ä»¶ç³»ç»Ÿæµ‹è¯•
# ============================================================================

def test_text_doc_loader_with_relative_path(tmp_path):
    """æµ‹è¯•ç›¸å¯¹è·¯å¾„"""
    old_cwd = os.getcwd()
    try:
        os.chdir(tmp_path)
        path = Path("relative.txt")
        path.write_text("Relative path test", encoding="utf-8")
        
        loader = TextDocLoader(file_path="relative.txt")
        documents = loader.load()
        
        assert documents[0].page_content == "Relative path test"
    finally:
        os.chdir(old_cwd)


def test_text_doc_loader_with_nested_directories(tmp_path):
    """æµ‹è¯•åµŒå¥—ç›®å½•"""
    nested_dir = tmp_path / "level1" / "level2" / "level3"
    nested_dir.mkdir(parents=True)
    path = nested_dir / "nested.txt"
    path.write_text("Nested file", encoding="utf-8")
    
    loader = TextDocLoader(file_path=str(path))
    documents = loader.load()
    
    assert documents[0].page_content == "Nested file"


def test_text_doc_loader_nonexistent_file():
    """æµ‹è¯•ä¸å­˜åœ¨çš„æ–‡ä»¶"""
    loader = TextDocLoader(file_path="/nonexistent/path/file.txt")
    with pytest.raises((FileNotFoundError, OSError)):
        loader.load()


# ============================================================================
# TextDocLoader Stream æµ‹è¯•
# ============================================================================

def test_text_doc_loader_stream_with_multiple_lines():
    """æµ‹è¯•æµè¯»å–å¤šè¡Œå†…å®¹"""
    loader = TextDocLoader()
    stream = io.StringIO("Line 1\nLine 2\nLine 3")
    
    documents = loader.fetch_content({"stream": stream})
    
    assert documents[0].page_content == "Line 1\nLine 2\nLine 3"


def test_text_doc_loader_stream_empty():
    """æµ‹è¯•ç©ºæµ"""
    loader = TextDocLoader()
    stream = io.StringIO("")
    
    documents = loader.fetch_content({"stream": stream})
    
    assert documents[0].page_content == ""


def test_text_doc_loader_stream_with_custom_metadata():
    """æµ‹è¯•å¸¦è‡ªå®šä¹‰å…ƒæ•°æ®çš„æµ"""
    loader = TextDocLoader()
    stream = io.StringIO("Test content")
    metadata = {
        "source": "custom_source",
        "author": "test_author",
        "timestamp": "2025-01-01",
        "category": "test"
    }
    
    documents = loader.fetch_content({"stream": stream, "metadata": metadata})
    
    assert documents[0].metadata["author"] == "test_author"
    assert documents[0].metadata["timestamp"] == "2025-01-01"
    assert documents[0].metadata["category"] == "test"


# ============================================================================
# Document ç±»æµ‹è¯•
# ============================================================================

def test_document_helpers():
    doc = Document(page_content="data", metadata={"category": "notes"}, category="notes")
    assert doc.has_metadata() is True
    assert doc.has_category() is True
    cloned = doc.copy_with_page_content("new")
    assert cloned.page_content == "new"
    assert cloned.metadata == doc.metadata
    assert cloned.metadata is not doc.metadata
    assert cloned.category == doc.category


def test_document_creation_basic():
    """æµ‹è¯•åŸºæœ¬æ–‡æ¡£åˆ›å»º"""
    doc = Document(page_content="test content")
    assert doc.page_content == "test content"
    assert doc.metadata == {}
    assert doc.category is None


def test_document_creation_with_metadata():
    """æµ‹è¯•å¸¦å…ƒæ•°æ®çš„æ–‡æ¡£åˆ›å»º"""
    metadata = {"source": "test.txt", "page": 1}
    doc = Document(page_content="content", metadata=metadata)
    
    assert doc.metadata["source"] == "test.txt"
    assert doc.metadata["page"] == 1


def test_document_creation_with_category():
    """æµ‹è¯•å¸¦åˆ†ç±»çš„æ–‡æ¡£åˆ›å»º"""
    doc = Document(page_content="content", category="technical")
    assert doc.category == "technical"
    assert doc.has_category() is True


def test_document_empty_content():
    """æµ‹è¯•ç©ºå†…å®¹æ–‡æ¡£"""
    doc = Document(page_content="")
    assert doc.page_content == ""
    assert len(doc.page_content) == 0


def test_document_metadata_modification():
    """æµ‹è¯•å…ƒæ•°æ®ä¿®æ”¹"""
    doc = Document(page_content="content", metadata={"key": "value"})
    doc.metadata["new_key"] = "new_value"
    
    assert doc.metadata["key"] == "value"
    assert doc.metadata["new_key"] == "new_value"


def test_document_copy_independence():
    """æµ‹è¯•æ–‡æ¡£å¤åˆ¶çš„ç‹¬ç«‹æ€§"""
    original = Document(
        page_content="original",
        metadata={"key": "value"},
        category="cat1"
    )
    copy = original.copy_with_page_content("modified")
    
    copy.metadata["key"] = "modified_value"
    copy.metadata["new_key"] = "new"
    
    assert original.metadata["key"] == "value"
    assert "new_key" not in original.metadata
    assert copy.page_content == "modified"
    assert original.page_content == "original"


def test_document_has_metadata_false():
    """æµ‹è¯•æ²¡æœ‰å…ƒæ•°æ®çš„æƒ…å†µ"""
    doc = Document(page_content="content")
    result = doc.has_metadata()
    assert result is not True or len(doc.metadata) == 0


def test_document_has_category_false():
    """æµ‹è¯•æ²¡æœ‰åˆ†ç±»çš„æƒ…å†µ"""
    doc = Document(page_content="content")
    assert doc.has_category() is False or doc.category is None


def test_document_long_content():
    """æµ‹è¯•é•¿æ–‡æ¡£å†…å®¹"""
    long_content = "test " * 10000
    doc = Document(page_content=long_content)
    assert len(doc.page_content) == len(long_content)


def test_document_with_complex_metadata():
    """æµ‹è¯•å¤æ‚å…ƒæ•°æ®"""
    metadata = {
        "source": "test.txt",
        "nested": {"key1": "value1", "key2": "value2"},
        "list": [1, 2, 3, 4, 5],
        "number": 123,
        "float": 3.14,
        "bool": True
    }
    doc = Document(page_content="content", metadata=metadata)
    
    assert doc.metadata["nested"]["key1"] == "value1"
    assert doc.metadata["list"] == [1, 2, 3, 4, 5]
    assert doc.metadata["number"] == 123
    assert doc.metadata["float"] == 3.14
    assert doc.metadata["bool"] is True


# ============================================================================
# RecursiveCharacterTextSplitter æµ‹è¯•
# ============================================================================

def test_splitter_basic_split():
    """æµ‹è¯•åŸºæœ¬åˆ†å‰²åŠŸèƒ½"""
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=10, max_chunk_overlap=2)
    text = "This is a test text for splitting"
    chunks = splitter.split_text(text)
    
    assert len(chunks) > 1
    for chunk in chunks:
        assert len(chunk) <= 10 or " " not in chunk[:10]


def test_splitter_no_overlap():
    """æµ‹è¯•æ— é‡å åˆ†å‰²"""
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=20, max_chunk_overlap=0)
    text = "word1 word2 word3 word4 word5"
    chunks = splitter.split_text(text)
    
    combined = "".join(chunks)
    assert combined.replace(" ", "") == text.replace(" ", "")


def test_splitter_with_overlap():
    """æµ‹è¯•æœ‰é‡å åˆ†å‰²"""
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=15, max_chunk_overlap=5)
    text = "alpha beta gamma delta epsilon"
    chunks = splitter.split_text(text)
    
    assert len(chunks) >= 2


def test_splitter_preserves_words():
    """æµ‹è¯•åˆ†å‰²ä¿æŒå•è¯å®Œæ•´æ€§"""
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=20, max_chunk_overlap=3)
    text = "word1 word2 word3 word4"
    chunks = splitter.split_text(text)
    
    for chunk in chunks:
        words = chunk.split()
        assert all(word in text.split() for word in words)


def test_splitter_single_long_word():
    """æµ‹è¯•å•ä¸ªè¶…é•¿å•è¯"""
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=10, max_chunk_overlap=2)
    text = "verylongwordthatexceedschunksize"
    chunks = splitter.split_text(text)
    
    assert len(chunks) >= 1


def test_splitter_empty_text():
    """æµ‹è¯•ç©ºæ–‡æœ¬åˆ†å‰²"""
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=10, max_chunk_overlap=2)
    chunks = splitter.split_text("")
    
    assert len(chunks) <= 1


def test_splitter_short_text():
    """æµ‹è¯•çŸ­æ–‡æœ¬ä¸éœ€åˆ†å‰²"""
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=100, max_chunk_overlap=10)
    text = "Short text"
    chunks = splitter.split_text(text)
    
    assert len(chunks) == 1
    assert chunks[0] == text


def test_splitter_multiline_text():
    """æµ‹è¯•å¤šè¡Œæ–‡æœ¬åˆ†å‰²"""
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=30, max_chunk_overlap=5)
    text = "Line 1 content here\nLine 2 content here\nLine 3 content here"
    chunks = splitter.split_text(text)
    
    assert len(chunks) >= 1


def test_splitter_with_documents(tmp_path):
    """æµ‹è¯•å¯¹æ–‡æ¡£åˆ—è¡¨è¿›è¡Œåˆ†å‰²"""
    path = tmp_path / "doc.txt"
    content = "a b c d e f g h i j k l m n o p q r s t u v w x y z"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=20, max_chunk_overlap=4)
    
    chunks = loader.load_and_split(splitter)
    
    assert len(chunks) > 1
    for chunk in chunks:
        assert chunk.metadata["source"] == str(path)


def test_splitter_preserves_metadata_across_chunks(tmp_path):
    """æµ‹è¯•åˆ†å‰²åå…ƒæ•°æ®ä¿æŒä¸€è‡´"""
    path = tmp_path / "meta_test.txt"
    content = "alpha " * 50
    path.write_text(content, encoding="utf-8")

    loader = TextDocLoader(str(path))
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=30, max_chunk_overlap=5)

    chunks = loader.load_and_split(splitter)

    sources = {chunk.metadata.get("source") for chunk in chunks}
    assert len(sources) == 1
    assert str(path) in sources


# ============================================================================
# MarkdownDocLoader åŸºç¡€æµ‹è¯•
# ============================================================================

def test_markdown_doc_loader_parses_front_matter(tmp_path):
    path = tmp_path / "frontmatter.md"
    path.write_text(
        "---\n"
        "title: Sample Doc\n"
        "tags:\n"
        "  - ai\n"
        "  - markdown\n"
        "---\n\n"
        "# Heading\n"
        "Content here.\n",
        encoding="utf-8",
    )

    loader = MarkdownDocLoader(str(path))
    documents = loader.load()

    assert len(documents) == 1
    doc = documents[0]
    assert doc.page_content.startswith("# Heading")
    assert doc.metadata["source"] == str(path)
    assert doc.metadata["front_matter"]["title"] == "Sample Doc"
    assert doc.metadata["front_matter"]["tags"] == ["ai", "markdown"]


def test_markdown_doc_loader_fetch_from_text():
    loader = MarkdownDocLoader()
    documents = loader.fetch_content(
        {
            "text": "---\ncategory: tests\n---\n\nHello",
            "metadata": {"extra": True},
        }
    )

    doc = documents[0]
    assert doc.page_content.strip() == "Hello"
    assert doc.metadata["front_matter"]["category"] == "tests"
    assert doc.metadata["extra"] is True
    assert doc.metadata["source"] == "inline"


def test_markdown_doc_loader_without_front_matter(tmp_path):
    path = tmp_path / "no_frontmatter.md"
    content = "# Title\n\nPlain text"
    path.write_text(content, encoding="utf-8")

    loader = MarkdownDocLoader(str(path))
    documents = loader.load()

    assert documents[0].page_content == content
    assert "front_matter" not in documents[0].metadata


def test_markdown_doc_loader_requires_source():
    loader = MarkdownDocLoader()
    with pytest.raises(ValueError):
        loader.load()


# ============================================================================
# PDFDocLoader åŸºç¡€æµ‹è¯•
# ============================================================================

def _create_pdf(tmp_path, name: str, pages: int = 2) -> str:
    pdf_path = tmp_path / name
    writer = PdfWriter()
    for _ in range(pages):
        writer.add_blank_page(width=200, height=200)
    with pdf_path.open("wb") as fh:
        writer.write(fh)
    return str(pdf_path)


def test_pdf_doc_loader_reads_pages(tmp_path):
    pdf_path = _create_pdf(tmp_path, "sample.pdf", pages=3)

    loader = PDFDocLoader(pdf_path)
    documents = loader.load()

    assert len(documents) == 3
    assert {doc.metadata["page_number"] for doc in documents} == {1, 2, 3}
    assert {doc.metadata["source"] for doc in documents} == {pdf_path}


def test_pdf_doc_loader_fetches_from_stream(tmp_path):
    pdf_path = _create_pdf(tmp_path, "stream.pdf")
    with open(pdf_path, "rb") as fh:
        payload = fh.read()

    loader = PDFDocLoader()
    documents = loader.fetch_content({"bytes": payload, "metadata": {"category": "pdf"}})

    assert len(documents) == 2
    for doc in documents:
        assert doc.metadata["source"] == "stream"
        assert doc.metadata["category"] == "pdf"


def test_pdf_doc_loader_requires_source():
    loader = PDFDocLoader()
    with pytest.raises(ValueError):
        loader.load()


def test_pdf_doc_loader_single_page(tmp_path):
    """æµ‹è¯•å•é¡µPDF"""
    pdf_path = _create_pdf(tmp_path, "single.pdf", pages=1)
    
    loader = PDFDocLoader(pdf_path)
    documents = loader.load()
    
    assert len(documents) == 1
    assert documents[0].metadata["page_number"] == 1


def test_pdf_doc_loader_many_pages(tmp_path):
    """æµ‹è¯•å¤šé¡µPDF"""
    pdf_path = _create_pdf(tmp_path, "many.pdf", pages=10)
    
    loader = PDFDocLoader(pdf_path)
    documents = loader.load()
    
    assert len(documents) == 10
    page_numbers = [doc.metadata["page_number"] for doc in documents]
    assert page_numbers == list(range(1, 11))


def test_pdf_doc_loader_metadata_consistency(tmp_path):
    """æµ‹è¯•PDFå…ƒæ•°æ®ä¸€è‡´æ€§"""
    pdf_path = _create_pdf(tmp_path, "meta.pdf", pages=5)
    
    loader = PDFDocLoader(pdf_path)
    documents = loader.load()
    
    for doc in documents:
        assert "source" in doc.metadata
        assert "page_number" in doc.metadata
        assert doc.metadata["source"] == pdf_path


def test_pdf_doc_loader_nonexistent_file():
    """æµ‹è¯•ä¸å­˜åœ¨çš„PDFæ–‡ä»¶"""
    loader = PDFDocLoader("/nonexistent/file.pdf")
    with pytest.raises((FileNotFoundError, OSError)):
        loader.load()


def test_pdf_doc_loader_with_custom_metadata(tmp_path):
    """æµ‹è¯•PDFè‡ªå®šä¹‰å…ƒæ•°æ®"""
    pdf_path = _create_pdf(tmp_path, "custom.pdf", pages=2)
    with open(pdf_path, "rb") as fh:
        payload = fh.read()
    
    loader = PDFDocLoader()
    custom_meta = {
        "author": "Test Author",
        "title": "Test Document",
        "category": "technical"
    }
    documents = loader.fetch_content({"bytes": payload, "metadata": custom_meta})
    
    assert len(documents) == 2
    for doc in documents:
        assert doc.metadata["author"] == "Test Author"
        assert doc.metadata["title"] == "Test Document"
        assert doc.metadata["category"] == "technical"


def test_pdf_doc_loader_bytes_empty():
    """æµ‹è¯•ç©ºå­—èŠ‚æ•°æ®"""
    loader = PDFDocLoader()
    with pytest.raises(Exception):
        loader.fetch_content({"bytes": b""})


# ============================================================================
# é›†æˆæµ‹è¯•
# ============================================================================

def test_integration_text_load_split_multiple_files(tmp_path):
    """æµ‹è¯•å¤šæ–‡ä»¶åŠ è½½å’Œåˆ†å‰²çš„é›†æˆ"""
    files = []
    for i in range(3):
        path = tmp_path / f"file_{i}.txt"
        content = f"File {i} content: " + "word " * 100
        path.write_text(content, encoding="utf-8")
        files.append(str(path))
    
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=50, max_chunk_overlap=10)
    all_chunks = []
    
    for file_path in files:
        loader = TextDocLoader(file_path)
        chunks = loader.load_and_split(splitter)
        all_chunks.extend(chunks)
    
    assert len(all_chunks) > 3
    sources = {chunk.metadata["source"] for chunk in all_chunks}
    assert len(sources) == 3


def test_integration_mixed_content_types(tmp_path):
    """æµ‹è¯•æ··åˆå†…å®¹ç±»å‹å¤„ç†"""
    # æ–‡æœ¬æ–‡ä»¶
    txt_path = tmp_path / "text.txt"
    txt_path.write_text("Text content", encoding="utf-8")
    
    # PDFæ–‡ä»¶
    pdf_path = _create_pdf(tmp_path, "doc.pdf", pages=2)
    
    # åŠ è½½æ–‡æœ¬
    txt_loader = TextDocLoader(str(txt_path))
    txt_docs = txt_loader.load()
    
    # åŠ è½½PDF
    pdf_loader = PDFDocLoader(str(pdf_path))
    pdf_docs = pdf_loader.load()
    
    assert len(txt_docs) == 1
    assert len(pdf_docs) == 2
    assert txt_docs[0].metadata["source"] == str(txt_path)
    assert all(doc.metadata["source"] == str(pdf_path) for doc in pdf_docs)


def test_integration_pipeline_with_filtering(tmp_path):
    """æµ‹è¯•å¸¦è¿‡æ»¤çš„å¤„ç†ç®¡é“"""
    path = tmp_path / "pipeline.txt"
    content = "alpha\nbeta\ngamma\ndelta\nepsilon\nzeta\neta\ntheta"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=20, max_chunk_overlap=3)
    chunks = loader.load_and_split(splitter)
    
    # è¿‡æ»¤åŒ…å«ç‰¹å®šå†…å®¹çš„å—
    filtered = [chunk for chunk in chunks if "alpha" in chunk.page_content or "beta" in chunk.page_content]
    
    assert len(filtered) >= 0
    for chunk in filtered:
        assert "alpha" in chunk.page_content or "beta" in chunk.page_content


def test_integration_document_reconstruction(tmp_path):
    """æµ‹è¯•æ–‡æ¡£é‡å»º"""
    path = tmp_path / "reconstruct.txt"
    original_content = "word " * 50
    path.write_text(original_content, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=30, max_chunk_overlap=5)
    chunks = loader.load_and_split(splitter)
    
    # éªŒè¯æ‰€æœ‰å—éƒ½æ¥è‡ªåŒä¸€ä¸ªæº
    assert len(chunks) > 1
    sources = {chunk.metadata["source"] for chunk in chunks}
    assert len(sources) == 1
    assert str(path) in sources


# ============================================================================
# è¾¹ç•Œæ¡ä»¶å’Œé”™è¯¯å¤„ç†æµ‹è¯•
# ============================================================================

def test_boundary_very_small_chunk_size(tmp_path):
    """æµ‹è¯•æå°çš„å—å¤§å°"""
    path = tmp_path / "small_chunk.txt"
    path.write_text("test content", encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=1, max_chunk_overlap=0)
    chunks = loader.load_and_split(splitter)
    
    assert len(chunks) >= 1


def test_boundary_zero_overlap(tmp_path):
    """æµ‹è¯•é›¶é‡å """
    path = tmp_path / "zero_overlap.txt"
    path.write_text("a b c d e f", encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=5, max_chunk_overlap=0)
    chunks = loader.load_and_split(splitter)
    
    assert len(chunks) >= 1


def test_boundary_large_overlap(tmp_path):
    """æµ‹è¯•å¤§é‡å """
    path = tmp_path / "large_overlap.txt"
    path.write_text("word " * 20, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=30, max_chunk_overlap=25)
    chunks = loader.load_and_split(splitter)
    
    assert len(chunks) >= 1


def test_error_invalid_file_path():
    """æµ‹è¯•æ— æ•ˆæ–‡ä»¶è·¯å¾„"""
    loader = TextDocLoader(file_path="")
    with pytest.raises((ValueError, FileNotFoundError, OSError)):
        loader.load()


def test_error_directory_instead_of_file(tmp_path):
    """æµ‹è¯•ä¼ å…¥ç›®å½•è€Œéæ–‡ä»¶"""
    loader = TextDocLoader(file_path=str(tmp_path))
    with pytest.raises((IsADirectoryError, PermissionError, OSError)):
        loader.load()


# ============================================================================
# æ€§èƒ½å’Œå‹åŠ›æµ‹è¯•
# ============================================================================

def test_performance_large_text_file(tmp_path):
    """æµ‹è¯•å¤§æ–‡æœ¬æ–‡ä»¶"""
    path = tmp_path / "large.txt"
    content = "line " * 100000
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    documents = loader.load()
    
    assert len(documents) == 1
    assert len(documents[0].page_content) == 500000


def test_performance_many_small_chunks(tmp_path):
    """æµ‹è¯•å¤§é‡å°å—åˆ†å‰²"""
    path = tmp_path / "many_chunks.txt"
    content = "w " * 10000
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    splitter = RecursiveCharacterTextSplitter(max_chunk_size=10, max_chunk_overlap=2)
    chunks = loader.load_and_split(splitter)
    
    assert len(chunks) > 100


def test_performance_many_pdf_pages(tmp_path):
    """æµ‹è¯•å¤§é‡PDFé¡µé¢"""
    pdf_path = _create_pdf(tmp_path, "large.pdf", pages=50)
    
    loader = PDFDocLoader(pdf_path)
    documents = loader.load()
    
    assert len(documents) == 50


# ============================================================================
# ç‰¹æ®Šåœºæ™¯æµ‹è¯•
# ============================================================================

def test_special_unicode_normalization(tmp_path):
    """æµ‹è¯•Unicodeè§„èŒƒåŒ–"""
    path = tmp_path / "unicode.txt"
    # åŒä¸€å­—ç¬¦çš„ä¸åŒUnicodeè¡¨ç¤º
    content = "cafÃ© cafÃ©"  # ä¸€ä¸ªç”¨ç»„åˆå­—ç¬¦ï¼Œä¸€ä¸ªç”¨é¢„ç»„åˆå­—ç¬¦
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    documents = loader.load()
    
    assert len(documents) == 1


def test_special_line_endings_unix(tmp_path):
    """æµ‹è¯•Unixé£æ ¼æ¢è¡Œç¬¦"""
    path = tmp_path / "unix.txt"
    content = "line1\nline2\nline3"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    documents = loader.load()
    
    assert "\n" in documents[0].page_content


def test_special_line_endings_windows(tmp_path):
    """æµ‹è¯•Windowsé£æ ¼æ¢è¡Œç¬¦"""
    path = tmp_path / "windows.txt"
    content = "line1\r\nline2\r\nline3"
    path.write_bytes(content.encode("utf-8"))
    
    loader = TextDocLoader(str(path))
    documents = loader.load()
    
    assert len(documents) == 1


def test_special_mixed_line_endings(tmp_path):
    """æµ‹è¯•æ··åˆæ¢è¡Œç¬¦"""
    path = tmp_path / "mixed.txt"
    content = "line1\nline2\r\nline3\rline4"
    path.write_bytes(content.encode("utf-8"))
    
    loader = TextDocLoader(str(path))
    documents = loader.load()
    
    assert len(documents) == 1


def test_special_tabs_and_spaces(tmp_path):
    """æµ‹è¯•åˆ¶è¡¨ç¬¦å’Œç©ºæ ¼æ··åˆ"""
    path = tmp_path / "tabs.txt"
    content = "word1\tword2  word3\t\tword4    word5"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    documents = loader.load()
    
    assert "\t" in documents[0].page_content


def test_special_repeated_separators(tmp_path):
    """æµ‹è¯•é‡å¤åˆ†éš”ç¬¦"""
    path = tmp_path / "repeated.txt"
    content = "word1     word2\n\n\nword3"
    path.write_text(content, encoding="utf-8")
    
    loader = TextDocLoader(str(path))
    documents = loader.load()
    
    assert documents[0].page_content.count(" ") >= 5


# ============================================================================
# å…ƒæ•°æ®é«˜çº§æµ‹è¯•  
# ============================================================================

def test_metadata_deep_nesting():
    """æµ‹è¯•æ·±å±‚åµŒå¥—å…ƒæ•°æ®"""
    metadata = {
        "level1": {
            "level2": {
                "level3": {
                    "level4": {"value": "deep"}
                }
            }
        }
    }
    doc = Document(page_content="content", metadata=metadata)
    
    assert doc.metadata["level1"]["level2"]["level3"]["level4"]["value"] == "deep"


def test_metadata_array_operations():
    """æµ‹è¯•æ•°ç»„å…ƒæ•°æ®æ“ä½œ"""
    metadata = {"tags": ["tag1", "tag2", "tag3"]}
    doc = Document(page_content="content", metadata=metadata)
    
    doc.metadata["tags"].append("tag4")
    assert len(doc.metadata["tags"]) == 4
    assert "tag4" in doc.metadata["tags"]


def test_metadata_null_values():
    """æµ‹è¯•ç©ºå€¼å…ƒæ•°æ®"""
    metadata = {"key1": None, "key2": "", "key3": 0, "key4": False}
    doc = Document(page_content="content", metadata=metadata)
    
    assert doc.metadata["key1"] is None
    assert doc.metadata["key2"] == ""
    assert doc.metadata["key3"] == 0
    assert doc.metadata["key4"] is False
